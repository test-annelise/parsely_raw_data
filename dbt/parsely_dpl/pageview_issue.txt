


-- multiple pageview events in a 1 hour bucket (assuming non-bot data)
select distinct
	count(distinct event_id) as num_events,
	action,
    apikey,
    session_id,
    visitor_site_id,
    visitor_ip,
    coalesce(metadata_canonical_url,url) as post_id,
    referrer
 from dbt_dev.parsely_base_events
 where action = 'pageview' and ua_browser <> 'Googlebot'
 group by action,
    apikey,
    session_id,
    visitor_site_id,
    visitor_ip,
    coalesce(metadata_canonical_url,url),
    referrer
having count(distinct event_id) > 1;

-- view of one of those duplicate but non-duplicate pageviews
select *
from dbt_dev.parsely_base_events
where /*action = 'pageview' and*/ session_id = '1013' and visitor_site_id = '88c51dec-c6ba-4268-af62-17b4c894f8bd' and (metadata_canonical_url = 'https://www.wsj.com/articles/trump-gets-blunt-at-the-united-nations-1506035079' or url = 'https://www.wsj.com/articles/trump-gets-blunt-at-the-united-nations-1506035079') and referrer ='https://www.wsj.com/news/opinion'
order by ts_action asc;


-- goal: sum of engaged_time for each pageview; 1 row per pageview (side note, is the pixel_depth supposed to update)
-- in order to get 2 pageviews, 1 with engaged_time = 19 and one with engaged time = 5, need to separate out the pageviews and sum the heartbeats according to time intervals
-- determine the previous and next timestamp for similar pageview links

 SELECT
    apikey,
    session_id,
    visitor_site_id,
    coalesce(metadata_canonical_url,url) as post_id,
    referrer,
    ts_action,
    LAG(ts_action, 1) OVER
    	(PARTITION BY
    		apikey,
		    session_id,
		    visitor_site_id,
		    coalesce(metadata_canonical_url,url),
		    referrer
		 ORDER BY ts_action) AS previous_tstamp,
	LAG(ts_action, 1) OVER
    	(PARTITION BY
    		apikey,
		    session_id,
		    visitor_site_id,
		    coalesce(metadata_canonical_url,url),
		    referrer
		 ORDER BY ts_action desc) AS next_tstamp
  FROM dbt_dev.parsely_base_events
  where action = 'pageview'
  and session_id = '1013' and visitor_site_id = '88c51dec-c6ba-4268-af62-17b4c894f8bd' and (metadata_canonical_url = 'https://www.wsj.com/articles/trump-gets-blunt-at-the-united-nations-1506035079' or url = 'https://www.wsj.com/articles/trump-gets-blunt-at-the-united-nations-1506035079') and referrer ='https://www.wsj.com/news/opinion'


-- how this was done (see pageviews.parsely_pageviews.sql model)
  select
      pv.event_id
      sum(engaged_time_inc) as engaged_time
  from pageview_events hb
  left join {{ref('parsely_parent_pageview_keys')}} pv using (apikey, session_id, visitor_site_id, pageview_post_id, referrer, ts_session_current)
  where action = 'heartbeat' and hb.ts_action >= pv.ts_action and (case when pv.next_pageview_ts_action is not null then hb.ts_action < pv.next_pageview_ts_action else true end)
  group by event_id

-- seeing this in action:
select *
from dbt_dev.parsely_pageviews
where session_id = '1013' and visitor_site_id = '88c51dec-c6ba-4268-af62-17b4c894f8bd' and (metadata_canonical_url = 'https://www.wsj.com/articles/trump-gets-blunt-at-the-united-nations-1506035079' or url = 'https://www.wsj.com/articles/trump-gets-blunt-at-the-united-nations-1506035079') and referrer ='https://www.wsj.com/news/opinion'

-- all is well and good
-- until a few new heartbeats from that last pageview come in the next incremental load
-- or a new pageview comes in with the same uniquely identifying info sprinkled with a few heartbeats that align with the previous pageview
-- then have to do the window function/LAG for all pageviews that match that criteria
-- update event_ids
-- update pageview and videoview details
-- update sessions and users

-- recommendation 1:
-- add a pageview_id and videoview_id

-- also recommendation 2:
-- for incremental loads - hold pageviews and heartbeats in a queue until with 100% certainty we have all surrounding possible buckets
-- what about outages and delays?
